{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Desktop\\\\PW_Projects\\\\Body_Mass_Index_from_Face_Images\\\\artifacts\\\\H_and_W_data_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Height\"]]\n",
    "y = df[\"Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "Standard_Scaler = StandardScaler()\n",
    "X_train_Standard_Scaler = Standard_Scaler.fit_transform(X_train)\n",
    "X_test_Standard_Scaler = Standard_Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Min_Max_Scaler = MinMaxScaler()\n",
    "X_train_Min_Max_Scaler = Min_Max_Scaler.fit_transform(X_train)\n",
    "X_test_Min_Max_Scaler = Min_Max_Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "Robust_Scaler = RobustScaler()\n",
    "X_train_Robust_Scaler = Robust_Scaler.fit_transform(X_train)\n",
    "X_test_Robust_Scaler = Robust_Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "MaxAbs_Scaler = MaxAbsScaler()\n",
    "X_train_MaxAbs_Scaler = MaxAbs_Scaler.fit_transform(X_train)\n",
    "X_test_MaxAbs_Scaler = MaxAbs_Scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_actual,y_predicted):\n",
    "    MAE = mean_absolute_error(y_actual,y_predicted)\n",
    "    MSE = mean_squared_error(y_actual,y_predicted)\n",
    "    RMSE = (MSE**0.5)\n",
    "    r2 = r2_score(y_actual,y_predicted)\n",
    "    return MAE,MSE,RMSE,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = {\"X_without_scaling\":[X_train,X_test],\n",
    "          \"X_Min_Max_Scaler\":[X_train_Min_Max_Scaler,X_test_Min_Max_Scaler],\n",
    "          \"X_Standard_Scaler\":[X_train_Standard_Scaler,X_test_Standard_Scaler],\n",
    "          \"X_Robust_Scaler\":[X_train_Robust_Scaler,X_test_Robust_Scaler],\n",
    "          \"X_MaxAbs_Scaler\":[X_train_MaxAbs_Scaler,X_test_MaxAbs_Scaler]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: X_without_scaling\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_without_scaling\n",
      "Prediction started with: X_without_scaling\n",
      "Prediction completed with: X_without_scaling\n",
      "Calculating metrics for X_without_scaling\n",
      "The Mean absolute error using data with, without_scaling is 22.62191360617924\n",
      "The Mean squared error using data with, without_scaling is 752.2607669818692\n",
      "The Root Mean squared error using data with, without_scaling is 27.427372586193325\n",
      "The R2_sqaure using data with, without_scaling is 0.1228734411908513\n",
      "==================================================================================================================================================\n",
      "Working with: X_Min_Max_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Min_Max_Scaler\n",
      "Prediction started with: X_Min_Max_Scaler\n",
      "Prediction completed with: X_Min_Max_Scaler\n",
      "Calculating metrics for X_Min_Max_Scaler\n",
      "The Mean absolute error using data with, Min_Max_Scaler is 22.62191360617924\n",
      "The Mean squared error using data with, Min_Max_Scaler is 752.2607669818692\n",
      "The Root Mean squared error using data with, Min_Max_Scaler is 27.427372586193325\n",
      "The R2_sqaure using data with, Min_Max_Scaler is 0.1228734411908513\n",
      "==================================================================================================================================================\n",
      "Working with: X_Standard_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Standard_Scaler\n",
      "Prediction started with: X_Standard_Scaler\n",
      "Prediction completed with: X_Standard_Scaler\n",
      "Calculating metrics for X_Standard_Scaler\n",
      "The Mean absolute error using data with, Standard_Scaler is 22.62191360617924\n",
      "The Mean squared error using data with, Standard_Scaler is 752.2607669818691\n",
      "The Root Mean squared error using data with, Standard_Scaler is 27.427372586193325\n",
      "The R2_sqaure using data with, Standard_Scaler is 0.12287344119085142\n",
      "==================================================================================================================================================\n",
      "Working with: X_Robust_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Robust_Scaler\n",
      "Prediction started with: X_Robust_Scaler\n",
      "Prediction completed with: X_Robust_Scaler\n",
      "Calculating metrics for X_Robust_Scaler\n",
      "The Mean absolute error using data with, Robust_Scaler is 22.62191360617924\n",
      "The Mean squared error using data with, Robust_Scaler is 752.2607669818692\n",
      "The Root Mean squared error using data with, Robust_Scaler is 27.427372586193325\n",
      "The R2_sqaure using data with, Robust_Scaler is 0.1228734411908513\n",
      "==================================================================================================================================================\n",
      "Working with: X_MaxAbs_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_MaxAbs_Scaler\n",
      "Prediction started with: X_MaxAbs_Scaler\n",
      "Prediction completed with: X_MaxAbs_Scaler\n",
      "Calculating metrics for X_MaxAbs_Scaler\n",
      "The Mean absolute error using data with, MaxAbs_Scaler is 22.62191360617924\n",
      "The Mean squared error using data with, MaxAbs_Scaler is 752.2607669818691\n",
      "The Root Mean squared error using data with, MaxAbs_Scaler is 27.427372586193325\n",
      "The R2_sqaure using data with, MaxAbs_Scaler is 0.12287344119085142\n",
      "==================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def Simple_Linear_Regression():\n",
    "    Linear_Regression = LinearRegression()\n",
    "    for key,value in X_data.items():\n",
    "        print(f'Working with: {key}')\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        Linear_Regression.fit(value[0],y_train)\n",
    "        print(f'Done with fitting the data using: {key}')\n",
    "        print(f'Prediction started with: {key}')\n",
    "        y_predicted = Linear_Regression.predict(value[1])\n",
    "        print(f'Prediction completed with: {key}')\n",
    "        print(f\"Calculating metrics for {key}\")\n",
    "        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\n",
    "        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\n",
    "        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\n",
    "        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\n",
    "        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\n",
    "        print(\"==================================================================================================================================================\")\n",
    "Simple_Linear_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: X_without_scaling\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_without_scaling\n",
      "Prediction started with: X_without_scaling\n",
      "Prediction completed with: X_without_scaling\n",
      "Calculating metrics for X_without_scaling\n",
      "The Mean absolute error using data with, without_scaling is 22.232329351398956\n",
      "The Mean squared error using data with, without_scaling is 737.2166649812551\n",
      "The Root Mean squared error using data with, without_scaling is 27.151734106337575\n",
      "The R2_sqaure using data with, without_scaling is 0.14041467422778642\n",
      "==================================================================================================================================================\n",
      "Working with: X_Min_Max_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Min_Max_Scaler\n",
      "Prediction started with: X_Min_Max_Scaler\n",
      "Prediction completed with: X_Min_Max_Scaler\n",
      "Calculating metrics for X_Min_Max_Scaler\n",
      "The Mean absolute error using data with, Min_Max_Scaler is 22.219416591650223\n",
      "The Mean squared error using data with, Min_Max_Scaler is 736.3227662054345\n",
      "The Root Mean squared error using data with, Min_Max_Scaler is 27.135267940549888\n",
      "The R2_sqaure using data with, Min_Max_Scaler is 0.1414569489170605\n",
      "==================================================================================================================================================\n",
      "Working with: X_Standard_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Standard_Scaler\n",
      "Prediction started with: X_Standard_Scaler\n",
      "Prediction completed with: X_Standard_Scaler\n",
      "Calculating metrics for X_Standard_Scaler\n",
      "The Mean absolute error using data with, Standard_Scaler is 22.206997302089583\n",
      "The Mean squared error using data with, Standard_Scaler is 735.3742817161815\n",
      "The Root Mean squared error using data with, Standard_Scaler is 27.117785339444325\n",
      "The R2_sqaure using data with, Standard_Scaler is 0.1425628698591831\n",
      "==================================================================================================================================================\n",
      "Working with: X_Robust_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Robust_Scaler\n",
      "Prediction started with: X_Robust_Scaler\n",
      "Prediction completed with: X_Robust_Scaler\n",
      "Calculating metrics for X_Robust_Scaler\n",
      "The Mean absolute error using data with, Robust_Scaler is 22.23272696372534\n",
      "The Mean squared error using data with, Robust_Scaler is 737.1592021442804\n",
      "The Root Mean squared error using data with, Robust_Scaler is 27.150675905845887\n",
      "The R2_sqaure using data with, Robust_Scaler is 0.14048167517036814\n",
      "==================================================================================================================================================\n",
      "Working with: X_MaxAbs_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_MaxAbs_Scaler\n",
      "Prediction started with: X_MaxAbs_Scaler\n",
      "Prediction completed with: X_MaxAbs_Scaler\n",
      "Calculating metrics for X_MaxAbs_Scaler\n",
      "The Mean absolute error using data with, MaxAbs_Scaler is 22.219188883582966\n",
      "The Mean squared error using data with, MaxAbs_Scaler is 736.2004275140059\n",
      "The Root Mean squared error using data with, MaxAbs_Scaler is 27.13301360914423\n",
      "The R2_sqaure using data with, MaxAbs_Scaler is 0.14159959428703317\n",
      "==================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def Random_Forest_Regressor():\n",
    "    Random_Forest_Regressor = RandomForestRegressor()\n",
    "    for key,value in X_data.items():\n",
    "        print(f'Working with: {key}')\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        Random_Forest_Regressor.fit(value[0],y_train)\n",
    "        print(f'Done with fitting the data using: {key}')\n",
    "        print(f'Prediction started with: {key}')\n",
    "        y_predicted = Random_Forest_Regressor.predict(value[1])\n",
    "        print(f'Prediction completed with: {key}')\n",
    "        print(f\"Calculating metrics for {key}\")\n",
    "        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\n",
    "        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\n",
    "        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\n",
    "        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\n",
    "        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\n",
    "        print(\"==================================================================================================================================================\")\n",
    "Random_Forest_Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: X_without_scaling\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with fitting the data using: X_without_scaling\n",
      "Prediction started with: X_without_scaling\n",
      "Prediction completed with: X_without_scaling\n",
      "Calculating metrics for X_without_scaling\n",
      "The Mean absolute error using data with, without_scaling is 22.092941309422596\n",
      "The Mean squared error using data with, without_scaling is 731.7445865190316\n",
      "The Root Mean squared error using data with, without_scaling is 27.050777928167456\n",
      "The R2_sqaure using data with, without_scaling is 0.14679504864284532\n",
      "==================================================================================================================================================\n",
      "Working with: X_Min_Max_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Min_Max_Scaler\n",
      "Prediction started with: X_Min_Max_Scaler\n",
      "Prediction completed with: X_Min_Max_Scaler\n",
      "Calculating metrics for X_Min_Max_Scaler\n",
      "The Mean absolute error using data with, Min_Max_Scaler is 22.092941309422603\n",
      "The Mean squared error using data with, Min_Max_Scaler is 731.7445865190316\n",
      "The Root Mean squared error using data with, Min_Max_Scaler is 27.050777928167456\n",
      "The R2_sqaure using data with, Min_Max_Scaler is 0.14679504864284532\n",
      "==================================================================================================================================================\n",
      "Working with: X_Standard_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Standard_Scaler\n",
      "Prediction started with: X_Standard_Scaler\n",
      "Prediction completed with: X_Standard_Scaler\n",
      "Calculating metrics for X_Standard_Scaler\n",
      "The Mean absolute error using data with, Standard_Scaler is 22.092941309422596\n",
      "The Mean squared error using data with, Standard_Scaler is 731.7445865190316\n",
      "The Root Mean squared error using data with, Standard_Scaler is 27.050777928167456\n",
      "The R2_sqaure using data with, Standard_Scaler is 0.14679504864284532\n",
      "==================================================================================================================================================\n",
      "Working with: X_Robust_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Robust_Scaler\n",
      "Prediction started with: X_Robust_Scaler\n",
      "Prediction completed with: X_Robust_Scaler\n",
      "Calculating metrics for X_Robust_Scaler\n",
      "The Mean absolute error using data with, Robust_Scaler is 22.0929413094226\n",
      "The Mean squared error using data with, Robust_Scaler is 731.7445865190316\n",
      "The Root Mean squared error using data with, Robust_Scaler is 27.050777928167456\n",
      "The R2_sqaure using data with, Robust_Scaler is 0.14679504864284532\n",
      "==================================================================================================================================================\n",
      "Working with: X_MaxAbs_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_MaxAbs_Scaler\n",
      "Prediction started with: X_MaxAbs_Scaler\n",
      "Prediction completed with: X_MaxAbs_Scaler\n",
      "Calculating metrics for X_MaxAbs_Scaler\n",
      "The Mean absolute error using data with, MaxAbs_Scaler is 22.092941309422603\n",
      "The Mean squared error using data with, MaxAbs_Scaler is 731.7445865190316\n",
      "The Root Mean squared error using data with, MaxAbs_Scaler is 27.050777928167456\n",
      "The R2_sqaure using data with, MaxAbs_Scaler is 0.14679504864284532\n",
      "==================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def Support_Vector_Regressor():\n",
    "    Support_Vector_Regressor = SVR()\n",
    "    for key,value in X_data.items():\n",
    "        print(f'Working with: {key}')\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        Support_Vector_Regressor.fit(value[0],y_train)\n",
    "        print(f'Done with fitting the data using: {key}')\n",
    "        print(f'Prediction started with: {key}')\n",
    "        y_predicted = Support_Vector_Regressor.predict(value[1])\n",
    "        print(f'Prediction completed with: {key}')\n",
    "        print(f\"Calculating metrics for {key}\")\n",
    "        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\n",
    "        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\n",
    "        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\n",
    "        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\n",
    "        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\n",
    "        print(\"==================================================================================================================================================\")\n",
    "Support_Vector_Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: X_without_scaling\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_without_scaling\n",
      "Prediction started with: X_without_scaling\n",
      "Prediction completed with: X_without_scaling\n",
      "Calculating metrics for X_without_scaling\n",
      "The Mean absolute error using data with, without_scaling is 23.000909041245134\n",
      "The Mean squared error using data with, without_scaling is 800.9658421502869\n",
      "The Root Mean squared error using data with, without_scaling is 28.301339935598225\n",
      "The R2_sqaure using data with, without_scaling is 0.06608393832947868\n",
      "==================================================================================================================================================\n",
      "Working with: X_Min_Max_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Min_Max_Scaler\n",
      "Prediction started with: X_Min_Max_Scaler\n",
      "Prediction completed with: X_Min_Max_Scaler\n",
      "Calculating metrics for X_Min_Max_Scaler\n",
      "The Mean absolute error using data with, Min_Max_Scaler is 22.99680553385214\n",
      "The Mean squared error using data with, Min_Max_Scaler is 800.8713492334539\n",
      "The Root Mean squared error using data with, Min_Max_Scaler is 28.299670479237985\n",
      "The R2_sqaure using data with, Min_Max_Scaler is 0.06619411587775914\n",
      "==================================================================================================================================================\n",
      "Working with: X_Standard_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Standard_Scaler\n",
      "Prediction started with: X_Standard_Scaler\n",
      "Prediction completed with: X_Standard_Scaler\n",
      "Calculating metrics for X_Standard_Scaler\n",
      "The Mean absolute error using data with, Standard_Scaler is 23.000909041245134\n",
      "The Mean squared error using data with, Standard_Scaler is 800.9658421502869\n",
      "The Root Mean squared error using data with, Standard_Scaler is 28.301339935598225\n",
      "The R2_sqaure using data with, Standard_Scaler is 0.06608393832947868\n",
      "==================================================================================================================================================\n",
      "Working with: X_Robust_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_Robust_Scaler\n",
      "Prediction started with: X_Robust_Scaler\n",
      "Prediction completed with: X_Robust_Scaler\n",
      "Calculating metrics for X_Robust_Scaler\n",
      "The Mean absolute error using data with, Robust_Scaler is 23.000909041245134\n",
      "The Mean squared error using data with, Robust_Scaler is 800.9658421502869\n",
      "The Root Mean squared error using data with, Robust_Scaler is 28.301339935598225\n",
      "The R2_sqaure using data with, Robust_Scaler is 0.06608393832947868\n",
      "==================================================================================================================================================\n",
      "Working with: X_MaxAbs_Scaler\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Done with fitting the data using: X_MaxAbs_Scaler\n",
      "Prediction started with: X_MaxAbs_Scaler\n",
      "Prediction completed with: X_MaxAbs_Scaler\n",
      "Calculating metrics for X_MaxAbs_Scaler\n",
      "The Mean absolute error using data with, MaxAbs_Scaler is 23.000909041245134\n",
      "The Mean squared error using data with, MaxAbs_Scaler is 800.9658421502869\n",
      "The Root Mean squared error using data with, MaxAbs_Scaler is 28.301339935598225\n",
      "The R2_sqaure using data with, MaxAbs_Scaler is 0.06608393832947868\n",
      "==================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "def KNN_Regressor():\n",
    "    KNN_Regressor = KNeighborsRegressor(n_neighbors=10)\n",
    "    for key,value in X_data.items():\n",
    "        print(f'Working with: {key}')\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        KNN_Regressor.fit(value[0],y_train)\n",
    "        print(f'Done with fitting the data using: {key}')\n",
    "        print(f'Prediction started with: {key}')\n",
    "        y_predicted = KNN_Regressor.predict(value[1])\n",
    "        print(f'Prediction completed with: {key}')\n",
    "        print(f\"Calculating metrics for {key}\")\n",
    "        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\n",
    "        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\n",
    "        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\n",
    "        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\n",
    "        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\n",
    "        print(\"==================================================================================================================================================\")\n",
    "KNN_Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def ANN_regression():\\n    for key,value in X_data.items():\\n        print(f\\'Working with: {key}\\')\\n        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\\n        model = tensorflow.keras.models.Sequential()\\n        model.add(tensorflow.keras.layers.Dense(128, activation=\\'relu\\', input_dim=X_train.shape[1]))\\n        model.add(tensorflow.keras.layers.Dense(64, activation=\\'relu\\'))\\n        model.add(tensorflow.keras.layers.Dense(1))\\n        model.compile(optimizer=\\'adam\\', loss=\\'mean_squared_error\\')\\n        model.fit(value[0],y_train, epochs=50, batch_size=32, shuffle=True)\\n        y_predicted = model.predict(value[1])\\n        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\\n        \\n        \\n        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\\n        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\\n        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\\n        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\\n        print(\"==================================================================================================================================================\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ANN_regression():\n",
    "    for key,value in X_data.items():\n",
    "        print(f'Working with: {key}')\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        model = tensorflow.keras.models.Sequential()\n",
    "        model.add(tensorflow.keras.layers.Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "        model.add(tensorflow.keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(tensorflow.keras.layers.Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(value[0],y_train, epochs=50, batch_size=32, shuffle=True)\n",
    "        y_predicted = model.predict(value[1])\n",
    "        MAE,MSE,RMSE,r2 = metrics(y_test,y_predicted)\n",
    "        \n",
    "        \n",
    "        print(f\"The Mean absolute error using data with, {key[2:]} is {MAE}\")\n",
    "        print(f\"The Mean squared error using data with, {key[2:]} is {MSE}\")\n",
    "        print(f\"The Root Mean squared error using data with, {key[2:]} is {RMSE}\")\n",
    "        print(f\"The R2_sqaure using data with, {key[2:]} is {r2}\")\n",
    "        print(\"==================================================================================================================================================\")\n",
    "    \n",
    "#ANN_regression()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Not enough memory resources are available to process this command.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pywrap_tensorflow_internal\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[39m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Not enough memory resources are available to process this command.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\notebooks\\Height_to_Weight_model_training_and_evaluation.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/PW_Projects/Body_Mass_Index_from_Face_Images/notebooks/Height_to_Weight_model_training_and_evaluation.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/PW_Projects/Body_Mass_Index_from_Face_Images/notebooks/Height_to_Weight_model_training_and_evaluation.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Dropout\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/PW_Projects/Body_Mass_Index_from_Face_Images/notebooks/Height_to_Weight_model_training_and_evaluation.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[39m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtraceback\u001b[39m.\u001b[39mformat_exc()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfor some common causes and solutions.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIf you need help, create an issue \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand include the entire stack trace above this error message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[39m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Not enough memory resources are available to process this command.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\PW_Projects\\Body_Mass_Index_from_Face_Images\\notebooks\\Height_to_Weight_model_training_and_evaluation.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/PW_Projects/Body_Mass_Index_from_Face_Images/notebooks/Height_to_Weight_model_training_and_evaluation.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m tensorflow\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = tensorflow.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
